---
title: 'Séance 4.3: Regression linéaire multiple - fin'
subtitle: "Diasgnostiques et violations des hypothèses"
author: "Visseho Adjiwanou, PhD."
institute: "SICSS - Montréal"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  beamer_presentation:
    colortheme: beaver
    fonttheme: structurebold
    theme: Antibes
  slidy_presentation: default
  ioslides_presentation: default
---


## Plan de présentation


- Violation des hypothèses
  - Interaction
  - Paramètres changeant: Interaction
  - Variables importantes omises


Validité des hypothèses
=============================================================

## Validité des hypothèses: analyse des résidus

- Présenter un graphique des résidus par rapport aux valeurs prédites (commande rvfplot dans Stata),

- Représenter les résidus par rapport à un régresseur spécifique (rvpplot)

- Ce que ce graphique montre est une indication de:
  - Non-linéarité
  - Non-normalité et
  - Hétéroscédasticité

## Résidus versus les valeurs prédites de Y

![](/Users/visseho/OneDrive - UQAM/Cours/Images_cours/validite1.png)

## Résidus versus une variable indépendante spécifique

![](/Users/visseho/OneDrive - UQAM/Cours/Images_cours/validite2.png)

Violation des hypothèses
=====================================================

## Violation des hypothèses

1. Non linéarité
2. Biais de spécification
3. paramètres non constants
4. Multicolinéarité
5. Hétéroscédasticité / perturbations autocorrélées 

## Violation de l'hypothèse de linéarité: et si Y et X sont non linéaires?

- Souvent en sciences sociales, la relation entre Y et X n'est pas connue à l'avance
- Cette relation n'est pas toujours linéaire
- Deux approches:
  - Utiliser la régression non linéaire
  - Transformez les dépendants et / ou les prédicteurs

## Violation de l'hypothèse de linéarité: et si Y et X sont non linéaires?

- Aucune relation linéaire entre le salaire et l'éducation 
- Besoin de transformation, log(salaire) = B0 + B1 * educ + erreur

![](/Users/visseho/OneDrive - UQAM/Cours/Images_cours/linearite.png)

- Les estimations des paramètres par MCO sont biaisées
- Les résultats sont également difficiles à interpréter

## Transformations et interprétations

- Il arrive souvent qu'on transforme la variable dépendante, ou indépendante ou les deux en cas de violation de la linéarité. C'est le cas entre par exemple salaire et niveau d'éducation. Dans ces cas, l'interprétation n'est pas la même. 

Comment faire? Transformations

1. Transformer uniquement des variables indépendantes

- Si la vraie relation est: $y = a + bx + cx^2 + \epsilon$
- Une transformation consiste à créer $z = x^2$
- et estimer la nouvelle équation $y = a + bx + cz + \epsilon$

## Transformations et interprétations

2. Transformer l'équation entière

Considérer l'équation de Cobb Douglas):

- $Y = b_0X^{bi}_i\epsilon$

- Prendre le logarithme des deux côtés donne:
$logY = log b_0 + b_1logX_1 + ...  + b_klogX_k + log\epsilon$

- Une régression doit être faite sur la variable de transformation et cette relation est à nouveau linéaire

3. Si la transformation n'est pas possible, d'autres types d'estimation doivent être utilisés, par exemple par les Maximum de vraisemblance


## Transformations et interprétations

- Voici quelques transformations usuelles et leur interprétation


![](/Users/visseho/OneDrive - UQAM/Cours/Images_cours/interpretation.png)


## Transformations et interprétations

1. Augmentation d'une unité de X entraîne un changement (augmentation ou diminution) de Y de B1 unité (effet sur Y)
2. Augmentation de 1% de X entraîne un changement de Y de (B1 / 100) unité (effet sur Y)
3. Augmentation d'une unité de X entraîne un changement de 100*B1% de Y (changement en % sur Y)
4. Augmentation de 1% de X entraine un changement de B1% de Y (changement en % sur Y)


## 2. Biais de spécification

1. Inclusion d'une variable indépendante non pertinente

- Relation vraie $Y = \beta_0 + \beta_1  X_1 + \beta_2  X_2 + \epsilon$
- Relation estimée: $Y = \beta_0 + \beta_1  X_1 + \beta_2  X_2 + \gamma Z +   \epsilon$
- Z n'est pas pertinent

**Conséquences**:

- Les paramètres estimés et leur matrice de variance-covariance sont **sans biais**
- À moins que le Z ne soit orthogonal aux autres X, la matrice de variance-covariance des paramètres estimés devient plus grande; l'estimateur OLS n'est pas aussi efficace.


## 2. Biais de spécification: 

2. Ommission de variables importantes

- Relation vraie $Y = \beta_0 + \beta_1  X_1 + \beta_2  X_2 + \epsilon$

- Relation estimée: $Y = \gamma_0 + \gamma_1  X_1  +   \epsilon$

- Conséquences: En général $\gamma_0$ et $\gamma_1$ sont biaisés


## 2. Biais de spécification: 

2. Ommission de variables importantes: sens du biais

- Relation vraie $Y = \beta_0 + \beta_1  X_1 + \beta_2  X_2 + \epsilon$

![](/Users/visseho/OneDrive - UQAM/Cours/Images_cours/biais_omission.png)

## 2. Biais de spécification: 
- Exemple: Prenez l'estimation de l'effet de l'éducation de la mère (X1) sur les résultats en math des enfants (Y). 
- Si vous estimez uniquement ce modèle sans l'inclusion de l'effet de l'éducation du père (X2), l'effet de l'éducation de la mère est sûrement biaisé. Dans quelle direction? Pour le savoir, vous devez utiliser les informations du tableau précédent.

Voir mon papier: 

https://www-sciencedirect-com.proxy.bibliotheques.uqam.ca/science/article/pii/S027795361730727X?via%3Dihub

https://nouvelles.umontreal.ca/en/article/2018/01/31/better-educated-men-healthier-women-and-mothers-in-the-developing-world/


## 2. Biais de spécification: 

On sait que:
- l'effet attendu de l'éducation du père sur les résultats en math des enfants est posititif ($\beta_2$) >0
- l'éducation de la mère est positivement correlé avec l'éducation du père: les gens de même niveau social se rassemblent (se marient). C'est ce qu'on appelle homophilie. Autrremnt dit, les femmes éduquées vont plus se marier avec les hommes éduqués, et vice versa. Donc corr(X1, X2) est positif. 
- Dans ce cas, l'effet de l'éducation de la mère est sûrement biaisé vers le haut, autrement dit, cet effet est plus élevé que l'effet qu'on aurait obtenu si on avait pris en compte l'éducation du père.

## 2. Biais de spécification: 

Trouver un cas où l'effet est sous-estimé

## 2. Biais de spécification

**Choix de variables**

- Avoir toutes les variables possibles : prolèmes avec les variables non pertinentes
- Sélection : problèmes d'omission de variables pertinentes

- Le choix doit être basé sur la théorie
- Si la théorie ne peut pas défendre l'utilisation d'une variable comme IV, elle ne doit pas être incluse comme variable indépendante


## 3. Paramètres non constants (changeants)

- L'effet de l'IV peut changer avec le temps ou d'une région à l'autre
- Les paramètres peuvent être déterminés par d'autres variables externes
- Ou peuvent-être aléatoires: régression multiniveaux
- Dans ce cas, l'hypothèse de paramètres constants doit être libérée
- Une régression spécifique dans chaque période ou dans chaque région doit être privilégiée.

## 3. Paramètres non constants (changeants)

- Test Chow pour les paramètres non constants
- Supposer que :
  - $Consommation_i = \beta_0 + \beta_1 Salaire_i + \beta_2 education_i + \epsilon_i$
  - Tester si les coefficients sont les mêmes dans la période 1 que dans la période 2
- Test Chow

  - Créez une variable fictive D = 1 pour l'observation de la période 2 et 0 pour l'observation de la période 1.
  - Exécutez la régression avec les termes d'interaction:
  
$Consommation_i = \beta_0 + \beta_1 Salaire_i + \beta_2 Education_i + \gamma_0 D + \gamma_1 (D * Salaire) + \gamma_2 (D * Education) + \epsilon_i$


## 3. Paramètres non constants (changeants)

- Si $\gamma_0 = \gamma_1 = \gamma_2$ alors les coefficients sont identiques pour les périodes un et 2.
- Utilisation du test F pour tester cette hypothèse

![](/Users/visseho/OneDrive - UQAM/Cours/Images_cours/chow_test.png)

- Où K est le nombre de paramètres, Ti nombre d'observations dans la période i (i = 1,2)


## Paramètres changeants: Interaction

- Vérifier si le coefficient de régression d'une variable indépendante (X1) varie en fonction des valeurs d'une autre variable indépendante (X2)
- On dit que X2 modère la relation entre X1 et la variable dépendante (Y)

- Plus fréquent entre les variables indépendantes qualitatives et continues
- Moins commun entre variable continue, mais plus facile à interpréter


## Exemple

- Interaction entre deux variables continues
- $Salaire = \beta_0 + \beta_1 Etudes + \beta_2 Experience + \epsilon$

![](/Users/visseho/OneDrive - UQAM/Cours/Images_cours/interaction1.png)

## Exemple

- Les effets des deux variables indépendantes sont indépendants l'un de l'autre

- $\beta_1^* = \delta(Salaire) / \delta(Etude)$ (variation de salaire due à une variation unitaire de Education) est indépendant de l'expérience et vice versa

- Interaction entre deux variables continues

$Wage = \beta_0 + \beta_1 Education + \beta_2 Experience + \epsilon$

- The effect of both IV are independent of each other

$\beta^*_1 = \delta(Wage)/\delta(Education)$  est indépendant de l'expérience et vice versa

## Exemple

- Cependant, l’effet de l’éducation peut être plus important chez les personnes ayant une expérience plus élevée que chez celles ayant moins d’expérience.
- Les deux droites ne sont pas parallèles

![](/Users/visseho/OneDrive - UQAM/Cours/Images_cours/interaction2.png)


## Exemple


$Salaire = \beta_0 + \beta_1Etude + \beta_2Experience + \beta_3Etude*Experience + \epsilon$

- Si $\beta_3> 0$ alors une année d’expérience supplémentaire augmente l’effet de l’éducation de $\beta_3$

- Si $\beta_3 <0$ alors une année d’expérience supplémentaire diminue l’effet de l’éducation de $\beta_3$

- Si $\beta_3$ = 0 alors Aucune interaction entre les deux variables


## Exemple

![](/Users/visseho/OneDrive - UQAM/Cours/Images_cours/interaction3.png)


## Interaction entre deux variables continues

$Salaire = \beta_0 + \beta_1 Etude + \beta_2 Experience + \beta_3 Etude * Experience + \epsilon$

- $\delta(Salaire) / \delta(Etude)$ =?
- $\delta(Salaire) / \delta(Etude) = \beta_1^* + \beta_3^* Experience$
- Il est donc clair que l’effet de l’étude sur les salaires dépend de la valeur de l’expérience.
- Avoir le carré d'une variable dans une régression a la même interprétation


## Interaction entre une variable continue et une variable dichotomique (dummy)

- Considérez $salaire = \beta_0 + \beta_1 Etude + \beta_2Male + \epsilon$
    - Aucun effet d'interaction

- $Salaire = \beta_0 + \beta_1 Etude + \beta_2 Homme + \beta_3 Homme * Etude + \epsilon$
    
- L'effet de l'éducation est différent pour les hommes et les femmes
- Pour les hommes: $salaire = \beta_0 + \beta_1 Etude + \beta_2 + \beta_3 Etude$
- Pour les femmes: $salaire = \beta_0 + \beta_2 Etude$
- Sans éducation, la différence entre le salaire des femmes et celui des hommes est $\beta_1$
- Chaque année supplémentaire d'études, augmente cette différence de $\beta_3$

## Interaction entre une variable continue et une variable dichotomique (dummy)

![](/Users/visseho/OneDrive - UQAM/Cours/Images_cours/interaction4.png)


## Interaction entre deux variables nominales

- Prenons l’effet de deux variables nominales sur le salaire:
    - Masculin
    - Secteur d'activités

- Sans interaction:
- $Salaire = \beta_0 + \beta_1 Masculin + \beta_2 Secondaire + \beta_3 Tertiaire + \epsilon$
    - Où les références sont Femme (Homme = 0) et Secteur = primaire
    
## Interaction entre deux variables nominales

Avec interaction avec 6 catégories à considérer:


![](/Users/visseho/OneDrive - UQAM/Cours/Images_cours/interaction5.png)

## Interaction entre deux variables nominales

Il faut bien considérer la référence et exécuter la régression appropriée

- $Salaire = \beta_0 + \beta_1 masculin * Prim + \beta_2 masculin * Sec + \beta_3 Fem * Prim + \beta_4 Fem * Sec + \beta_5 Fem * Ter + \epsilon$   

- Quelle est la référence?

- Devient complexe à interpréter
- Considérez une variable discrète comme variable continue si cette variable a plusieurs catégories

## 4. Multicollinéarité

- En réalité, la multicolinéraité parfaite est rare
  - La taille de l'échantillon est choisie toujours plus grande que l'IV
  - Les transformations de variables (mannequins) peuvent entraîner une multicolinéarité parfaite
    - Mais leur utilisation appropriée résout le problème

- Cependant, les relations linéaires approximatives entre les variables indépendantes sont courantes parmi les variables «économiques». C'est le cas de la multicolinéarité

## 4. Multicollinéarité

**Conséquences de la multicolinéarité**

- L'estimateur OLS en présence de multicolinéarité reste sans biais
- Le $R^2$ n'est pas affecté
- Cependant, la variance des estimations MCO des paramètres des variables colinéaires est assez grande

- Lorsque les régresseurs sont fortement corrélés, la plupart de leur variation est commune aux deux variables, laissant peu de variation propre à chaque variable - la procédure MCO a peu d'informations à utiliser pour faire ses estimations de coefficients.

## 4. Multicollinéarité

**Conséquences de la multicolinéarité**

- La multicolinéarité équivaut à avoir:
  - Très petit échantillon
  - Très peu de variation de la variable indépendante
  
![](/Users/visseho/OneDrive - UQAM/Cours/Images_cours/multicolinearite.png)

## 4. Multicollinéarité

**Comment détecter la multicolinéarité**

1. 
- Si les signes d'hypothèse ne sont pas trouvés dans les résultats de la régression, 
- ou si les variables pertinentes ont des valeurs t non significatives, 
- lorsque les résultats ont changé substantiellement avec la suppression de IV ou l'observation
  - **Malheureusement, ces raisons sont non nécessaires ou suffisantes**

2. Matrice de corrélation
  - Détecter la corrélation entre deux variables spécifiques
  - Des valeurs d'environ 0,8 ou 0,9 indiquent une corrélation élevée  
  
## 4. Multicollinéarité

**Comment détecter la multicolinéarité**

3. Facteurs d'inflation de variance, VIF

- $Y = \beta_0 + \beta_1X_{1i} + \beta_2X_{2i} + ... + \beta_kX_{ki} + \epsilon$

- Régresser chaque Xi sur les autres VI et obtenir le $R^2$ appelé $R^2_i$

- $VIF_i = \frac{1}{(1-R^2_i)^)}$

-  Si $VIF_i$ > 10 indique une colinéarité nuisible

## 4. Multicollinéarité

- Que faire en cas de multicolinéarité

- Rien Si 
  1. $R^2 > R^2_i$
  2. les statistiques t sont toutes supérieures à 2

- Incorporer des informations supplémentaires
  1. Obtenez plus de données
  2. Supprimer les variables (bonne si variable non pertinente)
  3. Crér un index à partir de l'analyse factorielle, de l'analyse en composante principale ou par tout autre moyen
  

## 5. Hétéroscédasticité / perturbations autocorrélées  

Hypothèse 3: les perturbations sont sphériques

- Variance uniforme $E(\epsilon^2_i) = \sigma^2$
- Non corrélés les uns aux autres $E(\epsilon_i\epsilon_j) = 0$, si i est différent de j

## 5. Hétéroscédasticité / perturbations autocorrélées  

Exemple avec trois personnes:
- Non sphériques

![](/Users/visseho/OneDrive - UQAM/Cours/Images_cours/non_spherique.png)

## 5. Hétéroscédasticité / perturbations autocorrélées  

- Sphérique

![](/Users/visseho/OneDrive - UQAM/Cours/Images_cours/Spherique.png)

## 5. Hétéroscédasticité / perturbations autocorrélées  

Si l'hypothèse 3 n'est pas respectée, la régression linéaire classique devient GLS: régression linéaire généralisée

- $\beta_{OLS}$ n'est plus approprié, remplacez par $\beta_{GLS}$
- $\beta_{OLS}$ sont non-baisés mais
- Ne sont pas efficaces (grande variance)

## 5. Hétéroscédasticité / perturbations autocorrélées  

- Cas particulier des perturbations autocorrélées
  - Plan d'échantillonnage en grappes : les individus du même cluster ne sont pas indépendants
  - Donnés collectés sur les élèves de la même classe

**Correction:**

- Option (Cluster) dans la régression pour corriger l'erreur standard
- Utilisation de la modélisation multiniveaux