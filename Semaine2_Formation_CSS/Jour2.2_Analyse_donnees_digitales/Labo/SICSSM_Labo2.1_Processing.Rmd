---
title: 'Labo 2.1: Analyse quantitative du texte'
subtitle: 'processing'
author: "Visseho Adjiwanou, PhD."
date: "`r format(Sys.time(), '%d %B %Y')`"
output: html_document
---


# Analyse de texte basé sur le dictionnaire

## Introduction

Nous utiliserons deux packages pour faire l'analyse quantitative du texte: tm qui est travaille sur les `document text-matrix` et *tidytext* qui utilise `tidy-data`. D'autres packages sont le [stm](https://cran.r-project.org/web/packages/stm/vignettes/stmVignette.pdf), [topicsmodels](https://cran.r-project.org/web/packages/topicmodels/vignettes/topicmodels.pdf), [quanteda](https://quanteda.io/)

## Type de données

1. Matrice Document-terme 

Une façon rapide d'explorer des données textuelles consiste à simplement compter les occurrences de chaque mot ou terme. Le nombre de fois qu'un mot particulier apparaît dans un document donné est appelé terme fréquence (tf). La statistique tf peut être résumée dans une matrice de termes de document, qui est un tableau rectangulaire avec des lignes représentant des documents et des colonnes représentant des termes uniques. L'élément (i, j) de cette matrice donne les décomptes du jième terme (colonne) dans le ième document (ligne). Nous pouvons également inverser les lignes et les colonnes et convertir une matrice de terme de document en une matrice de terme de document où les lignes et les colonnes représentent respectivement les termes et les documents.

![](/Users/visseho/OneDrive - UQAM/Cours/Images_cours/c11term_matrix.png)

  2. Tidy-data

L'utilisation de principes de données bien rangées est un moyen puissant de rendre la gestion des données plus facile et plus efficace, et cela n'est pas moins vrai lorsqu'il s'agit de traiter du texte. Comme décrit par Hadley Wickham (Wickham 2014), les données bien rangées ont une structure spécifique:

- Chaque variable est une colonne
- Chaque observation est une rangée
- Chaque type d'unité d'observation est un tableau

Nous définissons donc le format de texte bien rangé (tidy-text) comme étant une table avec un jeton (token) par ligne. Un jeton est une unité de texte significative, comme un mot, que nous souhaitons utiliser pour l'analyse, et la tokenisation est le processus de division du texte en jetons. Cette structure d'un jeton par ligne contraste avec la façon dont le texte est souvent stocké dans les analyses actuelles, peut-être sous forme de chaînes ou dans une matrice de termes de document. Pour l'exploration de texte ordonnée, le jeton qui est stocké dans chaque ligne est le plus souvent un seul mot, mais peut également être un n-gramme, une phrase ou un paragraphe. Dans le package tidytext, nous fournissons des fonctionnalités permettant de symboliser par des unités de texte couramment utilisées comme celles-ci et de les convertir en un seul terme par ligne.

![](/Users/visseho/OneDrive - UQAM/Cours/Images_cours/c11tidytextdata.png)

## 1. Dressons la table



```{r}

library(tidyverse)
library(tidytext)
library(tm)
library(maps)
library(SnowballC)
library(wordcloud)

```



## 2. Creating a corpus with the tm package

![](/Users/visseho/OneDrive - UQAM/Cours/Images_cours/c11prepocess.png)



```{r}

#load(url("https://cbail.github.io/Elected_Official_Tweets.Rdata"))

load(url("https://cbail.github.io/Trump_Tweets.Rdata"))

head(trumptweets)
  
trump_corpus <- Corpus(VectorSource(as.vector(trumptweets$text)))     #Create a corpus with every tweet becoming a file

trump_corpus
View((trump_corpus))

trump_corpus[["2"]][["content"]]    # See the contents of the file 2

# Removing numbers
trump_corpus <- tm_map(trump_corpus, content_transformer(removeNumbers))  

# Lower case
trump_corpus <- tm_map(trump_corpus,  content_transformer(tolower)) 

# Removing white space
trump_corpus <- tm_map(trump_corpus, content_transformer(stripWhitespace))

# Stemming
trump_corpus  <- tm_map(trump_corpus, content_transformer(stemDocument), language = "english")


# Document-term matrix
trump_DTM <- DocumentTermMatrix(trump_corpus, control = list(wordLengths = c(2, Inf)))
inspect(trump_DTM[1:5,1:8])

trump_DTM_matrix <- as.matrix(trump_DTM)

```


## 3. Tidy text


```{r}

tidy_trump_tweets <- 
  trumptweets %>% 
  select(created_at, text) %>% 
  unnest_tokens("word", text)     # Tokenise the data
  
tidy_trump_tweets  

tidy_trump_tweets %>% 
  count(word) %>% 
  arrange(desc(n))


  
```

### 4. Text pre-processing


```{r}

data("stop_words")
head(stopwords())

tidy_trump_tweets <- 
  tidy_trump_tweets %>% 
  anti_join(stop_words)

tidy_trump_tweets %>% 
  count(word) %>% 
  arrange(desc(n))

http_trump <- data.frame(word = c("https", "t.co", "amp", "rt"))
http_trump

tidy_trump_tweets <-
  tidy_trump_tweets %>% 
  anti_join(http_trump) 
  
tidy_trump_tweets  %>% 
  count(word) %>% 
  arrange(desc(n))

# Remove numbers
tidy_trump_tweets <- tidy_trump_tweets[-grep("\\b\\d+\\b", tidy_trump_tweets$word),]

# Once again tidytext automatically makes all words lower case.

# Removing whitespaces
tidy_trump_tweets$word <- gsub("\\s+","",tidy_trump_tweets$word)    

# Stemming
library(SnowballC)
  tidy_trump_tweets<-tidy_trump_tweets %>%
      mutate_at("word", funs(wordStem((.), language="en")))


# Document-term matrix
tidy_trump_DTM<-
  tidy_trump_tweets %>%
  count(created_at, word) %>%
  cast_dtm(created_at, word, n)  
  
```


# Analyse: Découverte des topics

Parmi les formes les plus élémentaires d'analyse quantitative de texte figurent les techniques de comptage de mots et les méthodes basées sur un dictionnaire. Ce labo couvrira ces deux sujets, ainsi que l'analyse des sentiments, qui est une forme d'analyse de texte basée sur un dictionnaire. 

## Fréquence des mots avec tidytext


```{r}

# Frequency of words

tidy_trump_tweets %>% 
 # select(word != "https") %>% 
  count(word, sort = TRUE)

tidy_trumptweets %>%          # Same as previously but with arrange
  count(word) %>% 
  arrange(desc(n))

#Graph of the most 20 words

top_20 <- 
  tidy_trump_tweets %>% 
  count(word, sort = TRUE) 

top_20 <- top_20[1:20, ]    
  
top_20  

top_20 %>% 
  ggplot(aes(x = word, y = n, fill = word)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  theme(axis.text = element_text(angle = 90, hjust = 1)) +
  ylab("Number of time a word appears in a tweet") +
  xlab("word") +
  guides(fill = FALSE)

tidy_trump_tweets_tfidf <-
  tidy_trump_tweets %>% 
  count(word, created_at) %>% 
  bind_tf_idf(word, created_at, n)
              

top_tfidf <- 
  tidy_trump_tweets_tfidf %>%
  arrange(desc(tf_idf))

top_tfidf




```


## Fréquence des mots avec tm

```{r}

# Wordcount   (Work with document-term Matrix)

head(tidy_trump_tweets)

dtm_trumptweets <- 
  tidy_trump_tweets %>% 
  count(created_at, word) #%>% 
#  cast_dtm(created_at, word, n)

dtm_trumptweets
head(dtm_trumptweets)

dtm_trumptweets %>% {
  wordcloud(.$word, .$n, max.words = 20)
}


inspect(trump_DTM[1:5,1:8])

trump_DTM %>% {
  wordcloud(.$word, .$n, max.words = 20)
}
```


## tf-idf
Bien que nous ayons déjà supprimé les «mots vides» très courants de notre analyse, il est courant dans l'analyse quantitative de texte d'identifier les mots inhabituels qui pourraient différencier un document des autres (cela deviendra particulièrement important lorsque nous passerons à des formes plus avancées de reconnaissance des formes dans le texte plus tard). La métrique la plus communément utilisée pour identifier ces mots inhabituels est «Term Frequency Frequency Inverse Document Frequency» (tf-idf). Nous pouvons calculer le tf-idf pour les tweets Trump basés sur des données en tidytext comme suit:

```{r}


tidy_trump_tfidf<- trumptweets %>%
  select(created_at,text) %>%
  unnest_tokens("word", text) %>%
  anti_join(stop_words) %>%
  count(word, created_at) %>%
  bind_tf_idf(word, created_at, n)


top_tfidf<-tidy_trump_tfidf %>%
  arrange(desc(tf_idf))

top_tfidf$word[1]



```

Le tfidf augmente à mesure qu'un terme apparaît dans un document, mais il est pondéré négativement par la fréquence globale des termes dans tous les documents de l'ensemble de données ou du corpus. En termes plus simples, le tf-idf nous aide à saisir quels mots sont non seulement importants dans un document donné, mais aussi distinctifs vis-à-vis du corpus plus large ou de l'ensemble de données tidytext.


# Références

https://www.tidytextmining.com/sentiment.html
https://www.datacamp.com/community/tutorials/sentiment-analysis-R
https://www.datacamp.com/community/tutorials/R-nlp-machine-learning
