---
title: 'Séance 1.2: Données digitales'
subtitle: "Collecte par grattage d'écran : screen-scraping"
author: "Visseho Adjiwanou, PhD."
institute: "SICSS - Montréal"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  slidy_presentation: default
  beamer_presentation:
    colortheme: beaver
    fonttheme: structurebold
    theme: Antibes
  ioslides_presentation: default
---



## Plan de présentation

- Collecte des données digitales à partir de grattage de site web
  1. Avec XPath
  2. Avec le selector de Gadget CSS (Cascading Style Sheets)
  3. Avec Selenium

- Conclusion  

## Remarques importantes

- Nous ferons recours dans cette partie du cours à plusieurs terminologies issues des sciences de l'informatique.
- Nous ne serons pas amener à comprendre de manière spécifique chaque thème technique avant de procéder.
- Dans la mesure du possible, je vous referai à d'autres ressources
- Comprenez juste que notre but est de pouvoir collecter des données de plus en plus abondantes sur le web de manière automatique et de pouvoir les utiliser dans nos recherches
- La question fondamentale demeure: pour quelles utilisations avez-vous besoin de ces données? En d'autres mots: c'est quoi votre question de recherche?

## Exemple de questions de recherche sur les groupes populistes au Québec

- Problématique:

  - L’étude du nationalisme et du populisme contemporains au Québec est intéressante à plusieurs égards. Aucun parti politique que l’on puisse clairement associer au populisme de droite ne bénéficie de sièges à l’Assemblée nationale, mais il est tout de même question dans les médias d’une montée des discours populistes (Bouchard, 2019 ; Cardinal, 2019) et d’un sentiment nationaliste ethnique (Croteau, 2013, 2015 ; Meagher, 2018). 

  - Les mouvements populistes non électoraux ne semblent pas non plus obtenir de grands succès : bien que des groupes comme La Meute, PEGIDA-Québec ou même le groupe ouvertement fasciste Atalante puisse occasionnellement faire parler d’eux dans les médias, la plus grande manifestation conjointe de ces organisations n’a réuni que quelques centaines de personnes (Béland, 2017).

## Exemple de questions de recherche

- Problématique:

- Au-delà de la question, largement débattue, des conditions de succès ou d’insuccès des partis et organisations populistes de droite, le cas québécois met aussi en évidence une autre question : comment le populisme se développe-t-il et fait-il sentir son influence dans ce contexte d’insuccès apparent des partis politiques et autres organisations de la droite populiste ?

## Exemple de questions de recherche

- Quels sont les marqueurs mobilisés dans les discours nationalistes, anti-immigration et islamophobes au Québec ? 
- Quels sont les liens entre marqueurs religieux, culturels et « ethno-raciaux » ?
- Quelles sont les conditions d’accession au plein statut de « Québécois » dans les discours nationalistes et anti-immigration? 
- Dans quelle mesure ces conditions sont-elles purement culturelles, et dans quelle mesure ont-elles un caractère plus ethnique ou racial ? 
- Quels rôles y jouent différentes appartenances religieuses, ou la notion de laïcité ?

## Exemple de questions de recherche

- Méthodologie

- Analyser les publications et commentaires provenant de pages Facebook des groupes "pro-laïcité", souvérainiste, "anti-immigration" et "anti-islamisation".


## Autres exemples tirés du texte de Matthews Salganik



Grattage de sites web
======================================================

## Qu'est-ce que le grattage d'écran?

- Le "screen-scraping" fait référence au processus d'extraction automatique des données de pages Web et souvent à une longue liste de sites Web qui ne peuvent pas être collectées à la main. 
- Comme l'illustre la figure ci-dessous, un programme de capture d'écran typique:
  a. charge le nom d'une page Web à extraire d'une liste de pages Web; 
  b. télécharge le site Web dans un format tel que HTML ou XML; 
  c. trouve une information souhaitée par l'auteur du code; et 
  d. place ces informations dans un format approprié, tel qu'un «bloc de données» (qui est un langage R parlant pour un jeu de données). 
  
- Le "screen-scraping"  peut également être utilisée pour télécharger d'autres types de contenu, tels que du contenu audiovisuel. 

## Qu'est-ce que le grattage d'écran?

![](/Users/visseho/OneDrive - UQAM/Cours/Images_cours/c10screen.png)


## Le screen-scraping est-il légal?

- Dans les premières années d’Internet, le grattage d’écran était une pratique très courante car il n’existait pas encore de normes juridiques très répandues concernant la protection des données sur Internet. 
- Cela a radicalement changé au cours des dernières décennies, lorsque la valeur des données sur les sites Web est devenue évidente et que des robots ou des programmes informatiques automatisés peuvent facilement causer des ravages en collectant des données sur des sites Web et en les réutilisant à des fins néfastes. 
- La toute première chose à considérer avant de "gratter" un site Web est de savoir si vous êtes autorisé à le faire. Pour ce faire, le plus simple consiste à consulter les «Conditions de service» (parfois abrégées en «Conditions») qui apparaissent souvent au bas d'une page Web. 

## Le screen-scraping est-il légal?

- De nos jours, la plupart des sites Web ont une politique «robots.txt» qui spécifie les règles relatives à la collecte automatisée de données sur le site, et un nombre croissant de sites n'autorisent pas de telles pratiques (en particulier des sites Web plus grands tels que Facebook, le New York Times ou Instagram. ). 

- Vous devriez consulter un avis juridique professionnel pour déterminer si vous avez l'autorisation de "gratter" un site Web.


1. Grattage avec Xpath
====================================================


## Lecture d'une page Web dans R

- Si vous avez identifié une page Web que vous souhaitez "gratter", la première étape dans la rédaction d'un programme de nettoyage d'écran consiste à télécharger le code source dans R. 
- Pour ce faire, nous allons installer le package rvest, créé par Hadley Wickham, qui fournit un certain nombre de fonctions très utiles pour le grattage d’écran.

- Vous devrez peut-être également installer le package selectr

```{r}
#install.packages("rvest")
#install.packages("selectr")
```

## Grattage d'une page simple

- Nous allons commencer par scrapper [cette page Web très simple de Wikipedia](https://en.wikipedia.org/wiki/World_Health_Organization_ranking_of_health_systems_in_2000). 

<!--- Je décris la page comme «simple», car elle ne comporte pas beaucoup de fonctionnalités interactives nécessitant des types de programmation Web sophistiqués, tels que javascript, avec lesquelles, comme nous le verrons dans un exemple ultérieur, il peut être particulièrement difficile de travailler.-->

- Voici à quoi ressemble la page Web liée ci-dessus lorsque nous la visitons via un navigateur tel que Explorer ou chrome:

![](/Users/visseho/OneDrive - UQAM/Cours/Images_cours/c10who.png)


## Grattage d'une page simple

- Mais ce n'est pas ce que la page Web ressemble réellement à notre navigateur. 
- Pour afficher le «code source» de la page Web, nous pouvons utiliser le menu déroulant de Chrome appelé «développeur», puis cliquer sur «Afficher la source». 
- Nous voyons ensuite la page dans sa forme la plus élémentaire, appelée fichier HTML, qui est un longue fichier contenant à la fois le texte de la page Web et une longue liste d'instructions sur la manière dont le texte, les images et les autres composants de la page Web doivent être restitués par le navigateur:

## Grattage d'une page simple

![](/Users/visseho/OneDrive - UQAM/Cours/Images_cours/c10code.png)

## Grattage d'une page simple

- Pour télécharger le code source dans R, nous pouvons utiliser la fonction **read_html** du package **rvest** que nous venons d'installer ci-dessus. 

```{r}
#install.packages("rvest")
library(tidyverse)
library(rvest)

wikipedia_page <- read_html("https://en.wikipedia.org/wiki/World_Health_Organization_ranking_of_health_systems_in_2000")

wikipedia_page

```


## Analyse HTML (Parsing HTML)

- La partie la plus difficile du nettoyage d’écran consiste peut-être à extraire l’information que vous souhaitez extraire du fichier html. 
- Cela pose un défi car ces informations sont presque toujours cachées au plus profond du code source, ce qui peut être très difficile à parcourir. 
- Heureusement, comme le montre la figure ci-dessous, le code source d'une page Web, tel que HTML ou XML, possède une structure «arborescente» qui vous permet d'affiner progressivement la partie de la page Web où réside l'information que vous souhaitez.

## Analyse HTML (Parsing HTML)

![](/Users/visseho/OneDrive - UQAM/Cours/Images_cours/c10html.png)


## Analyse HTML (Parsing HTML)

- Pour savoir où se trouvent les données souhaitées dans cette structure arborescente, nous pouvons utiliser un outil pratique dans Chrome, appelé "Outils de développement" (Developper Tools):

![](/Users/visseho/OneDrive - UQAM/Cours/Images_cours/c10developper.png)

## Analyse HTML (Parsing HTML)

- Cet outil vous fournit une interface interactive où vous pouvez afficher la page Web à côté du code source. 
- Lorsque vous cliquez avec le bouton droit de la souris sur la partie de la page Web qui vous intéresse et que vous choisissez «inspecter», l'outil de développement met en surbrillance la partie du code source où se trouvent les informations souhaitées. 
- Dans la figure ci-dessous, j'ai mis en évidence le tableau décrivant les indicateurs de santé pour les différents pays décrits sur la page Wikipedia liée ci-dessus.

## Analyse HTML (Parsing HTML)

![](/Users/visseho/OneDrive - UQAM/Cours/Images_cours/c10dev2.png)

## Analyse HTML (Parsing HTML)

- Lorsque j'ai inspecté la partie de la page Web que j'essayais de gratter en cliquant avec le bouton droit de la souris, la partie du code HTML ci-dessous est surlignée:

![](/Users/visseho/OneDrive - UQAM/Cours/Images_cours/c10dev3.png)


<!--Parfois, trouver la partie exacte du code dans laquelle se trouvent les données que vous souhaitez conserver nécessite des essais et des erreurs. Dans ce cas, j'ai découvert que je devais sélectionner une ligne précédente dans le code pour identifier la table entière que je voulais gratter au lieu d'une seule ligne.-->

## Analyse HTML (Parsing HTML)

- L'étape suivante consiste à identifier une chaîne de chiffres et de lettres appelée «Xpath» pour cette partie du code source. 
- Le Xpath décrit la partie précise du code HTML où sont logées les données que je veux. 
- J'identifie le "Xpath" en cliquant avec le bouton droit de la souris sur la section en surbrillance du volet des outils de développement qui affiche le code html:

![](/Users/visseho/OneDrive - UQAM/Cours/Images_cours/c10dev4.png)

## Analyse HTML (Parsing HTML)

- Maintenant que j'ai le Xpath, je peux utiliser ces informations pour affiner ma recherche dans le fichier HTML pour le tableau que je veux extraire à l'aide de la fonction **html_nodes** qui transmet le Xpath en tant qu'argument, comme illustré dans le code ci-dessous:

```{r}

#Section_wikipedia2 <- html_node(wikipedia_page, xpath = '//*[@id="mw-content-text"]/div/table[2]')

# //*[@id="mw-content-text"]/div[1]/table

Section_wikipedia2 <- html_node(wikipedia_page, xpath = '//*[@id="mw-content-text"]/div[1]/table')


head(Section_wikipedia2)
```

## Analyse HTML (Parsing HTML)

<!-- Comme le montre la sortie ci-dessus, je me rapproche maintenant, mais je ne suis pas encore tout à fait où je veux être. -->

- La partie supérieure de la sortie m'indique que les données sont au format table (plutôt qu'au format texte), je vais donc utiliser la fonction **html_table** pour extraire la table incorporée dans le code html:

```{r}

health_rankings <- html_table(Section_wikipedia2, fill = T)

head(health_rankings[, (1:2)])

```


## Remarques

- Comme le montre la sortie ci-dessus, je dispose enfin des données souhaitées sous la forme d'un bloc de données ou d'un jeu de données appelé «health_rankings». 
- Il s'agissait toutefois d'un travail fastidieux, mais rappelez-vous qu'il s'agissait d'un type de page Web très simple. 
- En outre, il convient de noter que les groupes de pages Wikipedia ont un format très similaire, ce qui les rend très propices au grattage d'écran. 
- Si, en revanche, je cherchais une liste de sites appartenant à plusieurs domaines, chacun ayant sa propre structure complexe, je pourrais passer des jours ou des semaines à écrire du code pour le gratter. 

## Remarques

- Dans ce scénario, de nombreuses personnes peuvent trouver qu'il est plus facile de collecter les données manuellement ou d'engager du personnel sur Amazon Mechanical Turk pour extraire les informations que vous souhaitez peut-être d'une liste de pages Web.

- Une dernière remarque: souvent, la page Web que vous souhaitez gratter contient du XML au lieu de HTML (ou en plus du HTML). Dans ce cas, le package rvest dispose d'une série de fonctions permettant d'analyser XML, telles que **xml** et **xml_node**.

## En résumé

Pour faire un grattage de sites :

1. Lire la page: read_html
2. Trouver la partie qui vous concerne et copier le xpath: developpers_tools
3. Sélectionner cette partie avec html_node
4. Télécharger le sous le format approprié html_table si table


```{r}
#install.packages("rvest")
library(tidyverse)
library(rvest)

wikipedia_page <- read_html("https://en.wikipedia.org/wiki/World_Health_Organization_ranking_of_health_systems_in_2000")

section_table <- html_node(wikipedia_page, xpath = '//*[@id="mw-content-text"]/div[1]/table')

base_de_donnee_wiki <-html_table(section_table, fill = TRUE)

```

## A vous de jouer

1. Gratter le site web de votre école/département pour collecter les informations sur les professeur.es

2. Gratter le site web du département de Sociologie de l'uqam pour collecter les informations sur les professeurs
  - https://sociologie.uqam.ca/corps-professoral/professeurs-es/


```{r}

page_socio_uqam <- read_html("https://sociologie.uqam.ca/corps-professoral/professeurs-es/")

prof_socio <- html_node(page_socio_uqam, xpath = '//*[@id="post-967"]/div')
prof_tableau <- html_text(prof_socio)
prof_tableau

```

2. Télécharger le ranking des films
  - https://www.imdb.com/title/tt1490017/


2. Grattage avec le selecteur CSS
====================================================

## Collecte avec le sélecteur CSS

- Il est souvent le cas, en particulier avec des pages Web plus complexes, que les procédures décrites ci-dessus ne fonctionnent pas. 
- Dans ce cas, il est utile de connaître une alternative au code Xpath décrit ci-dessus: le «sélecteur CSS», qui est un autre extrait de code qui vous aide à trouver le nugget de HTML que vous souhaitez extraire d'une page Web.

## Collecte avec le sélecteur CSS

- Voyons un exemple d’extraction d’une liste «d’évènements» de la page Web principale de l’Université Duke. 
- Si vous visitez cette page, vous verrez que l'exemple est considérablement plus complexe que l'exemple de Wikipedia susmentionné. Il y a non seulement plus de types d'informations présentés sur la page, mais de multiples façons d'y accéder. 
- Dans notre exemple hypothétique, les informations que nous souhaitons changer quotidiennement (la liste des événements majeurs qui se produiront chez Duke ce jour-là).

## Collecte avec le sélecteur CSS

- Pour identifier le sélecteur CSS, nous allons utiliser un plug-in Chrome très répandu appelé le sélecteur Gadget. 
- Il s'agit d'un outil interactif, semblable à l'outil de développeur Chrome décrit ci-dessus, qui permet d'interagir avec une page Web afin de révéler le sélecteur CSS. 
- Après l’installation de l’outil, une petite icône apparaît dans la barre d’outils de Google Chrome, à savoir une main tenant un microscope. 
- Lorsque vous cliquez sur cet outil, un nouveau volet apparaîtra en bas de la fenêtre de votre navigateur et révélera le CSS Selector. Il peut également être utilisé pour identifier le Xpath. Ce volet est illustré au bas de la figure ci-dessous:

## Collecte avec le sélecteur CSS

- Cliquez sur l'une des parties jaunes jusqu'à ce qu'il ne reste que la partie verte que vous avez initialement sélectionnée. 
- Lorsque vous cliquez dessus, vous pouvez constater que le sélecteur CSS sélectionné en bas de la page change. 
- Cela peut prendre quelques essais et erreurs; le gadget sélecteur est imparfait. 

## Collecte avec le sélecteur CSS

- Cela fonctionne mieux lorsque vous essayez de cliquer sur différentes parties surlignées en jaune, puis d'essayer les sélecteurs CSS résultants qu'il identifie de manière itérative à l'aide du code ci-dessous.

- Une fois que nous avons identifié le sélecteur CSS que nous pensons attaché à l’information souhaitée, nous pouvons le transmettre comme argument dans la fonction html_nodes de rvest, comme suit:



```{r}
library(selectr)

site_wiki <- read_html("https://en.wikipedia.org/wiki/World_Health_Organization_ranking_of_health_systems_in_2000")
section_wiki <- html_node(site_wiki, css = '.jquery-tablesorter td , .headerSort')
?html_node()
#tableau_wiki <- html_table(section_wiki)
#tableau_wiki

#site_uqam <- read_html("https://sociologie.uqam.ca/corps-professoral/professeurs-es/")
#section_site <- html_node(site_uqam, css = ".infos , .nom")
#nom_texte <- html_text(section_site)
#nom_texte[1]

```


## Collecte avec le sélecteur CSS


```{r}

uqam_page <- read_html("https://uqam.ca")

Section_uqam <- html_node(uqam_page, css = '.GoldenRetriever-content')

Section_uqam1 <- html_node(uqam_page, css = '.GoldenRetriever-item:nth-child(3) .GoldenRetriever-content')

Section_uqam[[1]]
Donnees_uqam <- html_text(Section_uqam)
Donnees_uqam
```


## Exemple 1 : Wikipedia with SelectorGadget

```{r}

wikipedia_page <- read_html("https://en.wikipedia.org/wiki/World_Health_Organization_ranking_of_health_systems_in_2000")

Section_wiki <- html_node(wikipedia_page, css = 'td , a .thumbborder , .headerSort')


text_wiki <- html_text(Section_wiki)
text_wiki

```


## Remarques


- Comme le montre la sortie ci-dessus, nous avons maintenant collecté des informations sur les événements souhaités. 
- Notez, cependant, qu'il inclut des caractères en désordre avec des barres obliques inverses, ts et ns. 
- Celles-ci s'appellent des «balises html», et nous avons apprendre à les nettoyer dans un prochain tutoriel sur le traitement de base du texte en R.

- Maintenant vous essayez !!!

## Ressources

1. https://thinkr.fr/rvest/ 
2. https://www.datacamp.com/community/tutorials/r-web-scraping-rvest
3. https://blog.rstudio.com/2014/11/24/rvest-easy-web-scraping-with-r/
4. https://selectorgadget.com/
5. https://cran.r-project.org/web/packages/rvest/vignettes/selectorgadget.html



3. Screen-scraping avec Selenium
=================================================


## Screen-scraping avec Selenium

- Parfois, notre intérêt à gratter les pages Web implique plus d’interaction avec un navigateur Web. 
- Par exemple, supposons que nous voulions visiter les pages Web principales d’une série d’universités et lancer une recherche sur «la science des données». 
- Cela nécessiterait de saisir des données dans une barre de recherche sur une série de sites, puis de collecter les données obtenues. 

## Screen-scraping avec Selenium

- Afin d'accomplir de telles tâches, il est parfois utile d'automatiser l'intégralité de votre navigateur Web. 
- Cela signifie que nous écrirons du code qui indiquera à notre ordinateur: 
  a. d'ouvrir un navigateur Web; 
  b. charger une page Web; 
  c. interagir avec la page Web en cliquant sur la barre de recherche et en entrant du texte; et 
  d. télécharger les données résultantes.

## Screen-scraping avec Selenium

- RSelenium est un puissant package pour automatiser des tâches sur votre ordinateur. 
- Il peut également être utilisé pour automatiser votre navigateur et effacer des informations. 
- Pour écrire du code pour Rselenium, vous devez saisir les touches du clavier, telles que «Tabulation», «Ctrl» ou «Entrée», puis les intercaler avec le texte que vous souhaitez saisir

<!-- Dans l'exemple ci-dessus, nous pourrions passer le texte «données scientifiques».-->

## Screen-scraping avec Selenium

- Malheureusement, installer RSelenium peut être un peu compliqué, car il nécessite une interface plus sophistiquée entre votre session RStudio et votre système d’exploitation par rapport au paquet R typique. 
- Les instructions qui suivent vous aideront à installer R Selenium sur Mac OSX.13.4. 
- Tout d'abord, vous devrez installer un autre logiciel appelé Java SE Development Kit. Choisissez la version la plus appropriée pour votre machine sur ce [lien](https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html). 
- Ensuite, vous devrez télécharger Docker et l’installer sur votre ordinateur. 
- Lorsque vous êtes invité à entrer vos identifiants de connexion, vous devrez fournir le nom d'utilisateur que vous avez configuré pour télécharger Docker, pas votre adresse électronique. 
- Une fois que cela sera fait, Docker sera disponible pour RSelenium à l’aide de la commande suivante:


## Screen-scraping avec Selenium

```{r}

system('docker run -d -p 4445:4444 selenium/standalone-chrome')

```


## En résumé:
1. [Java SE Development Kit](https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html)
2. télécharcher et installer [Docker](https://www.docker.com/products/docker-desktop)
3. Lancer system('docker run -d -p 4445:4444 selenium/standalone-chrome')


## Screen-scraping avec Selenium

- Ensuite, vous devrez installer RSelenium
<!--, qui est enfin à nouveau hébergé sur CRAN. (Pendant quelques mois plus tôt cette année, ce n'était pas le cas, mais vous avez pu utiliser une solution de contournement pour l'installer à partir de Github à l'aide du package devtools.)-->
- Après avoir téléchargé et chargé le package, nous démarrons le serveur Selenium et ouvrons Chrome avec commandes ci-dessous 
- Si vous n'avez pas installé Chrome, vous pouvez démarrer Firefox à la place en utilisant la commande suivante `rD <- rsDriver (browser = "firefox")`.

## Screen-scraping avec Selenium

- Remarque: Lorsque vous exécutez la fonction **rsDriver** pour configurer une session RSelenium, RSelenium choisira la dernière version disponible de chromedriver par défaut. 
- Souvent, cette dernière version reflète une version développeur de Chrome qui n'est peut-être pas encore disponible, même en version bêta, ce qui entraîne une incompatibilité qui empêchera RSelenium de fonctionner. 
- Pour résoudre ce problème, utilisez **binman :: list_versions ("chromedriver")** pour identifier la version de chromedriver correspondant à votre version installée de Chrome 

## Screen-scraping avec Selenium

- Dans Chrome, sélectionnez "À propos de Google Chrome" pour identifier votre version installée. 
- Une fois identifiée, spécifiez cette version comme valeur de l'argument chromever dans rsDriver. 
- Merci à cette page [Stack Overflow](https://stackoverflow.com/questions/55201226/session-not-created-this-version-of-chromedriver-only-supports-chrome-version-7/56173984#5617398) pour le correctif!

## Screen-scraping avec Selenium

```{r, eval=FALSE}


#install.packages("RSelenium")
library(RSelenium)

# Check available versions of chromedriver
#binman::list_versions("chromedriver")          # Ne marche pas

# start a Selenium server
rD <- rsDriver(browser = c("chrome"), chromever = "85.0.4183.87")


# open the browser
remDr <- rD$client

```

## Screen-scraping avec Selenium

- Maintenant, nous pouvons lancer la page Web Duke ci-dessus comme suit:

```{r, eval=FALSE}

remDr$navigate("https://www.duke.edu")

```

## Screen-scraping avec Selenium

- Ensuite, nous devons utiliser le gadget de sélection ou une autre méthode pour identifier la partie de la page Web où se trouve la barre de «recherche». 
- Après quelques essais et erreurs, j'ai découvert que le sélecteur CSS était **fieldset input**:

```{r, eval=FALSE}

search_box <- remDr$findElement(using = 'css selector', 'fieldset input')

```

## Screen-scraping avec Selenium

- Vous verrez maintenant que la barre de recherche est mise en surbrillance dans votre navigateur. 
- La seule chose qui reste à faire est de demander à RSelenium de taper «data science» dans la zone de requête, puis d'appuyer sur le bouton de retour à l'aide de la fonction **sendKeysToElement**. 
- Le code du trait de touche "Entrée" ou "Retour" est le deuxième argument que nous transmettons à la fonction susmentionnée (\ uE007)


```{r, eval=FALSE}

search_box$sendKeysToElement(list("data science", "\uE007"))



```

## Screen-scraping avec Selenium

- Ce code devrait entraîner une recherche de «data science» sur la page Web de Duke.

- Et finalement, vous quittez l'application

```{r, eval=FALSE}
remDr$close() 
rD$server$stop()
```



## Ressources

- https://freakonometrics.hypotheses.org/tag/rselenium
- http://rpubs.com/johndharrison/RSelenium-Basics
- http://cran.nexr.com/web/packages/RSelenium/vignettes/OCRUG-webinar.html


Conclusion
===================


## Scraping d'écran dans une boucle

- Quelle que soit l'approche de grattage d'écran que vous utilisez, vous souhaiterez souvent gratter non pas une seule page, mais plusieurs pages. 
- Dans ce cas, vous pouvez simplement intégrer l'une des techniques ci-dessus dans une boucle **for** qui lit une liste de pages Web que vous souhaitez gratter. 
- Encore une fois, sachez que les sites Web ont souvent des structures différentes, de sorte que la façon dont vous extrayez des informations sur un site peut être très différente d'un autre. 
- C'est l'une des principales raisons pour lesquelles le grattage d'écran peut être difficile - et aussi une autre raison pour laquelle le grattage d'écran est passé de mode en plus des problèmes juridiques soulignés ci-dessus. 

## Scraping d'écran dans une boucle

- Si vous continuez à gratter une grande liste de pages Web, vous souhaiterez peut-être intégrer une «gestion des erreurs» dans votre code, afin qu'il ne se casse pas à chaque fois que votre code ne fonctionne pas. 
- Notez que certaines erreurs peuvent également être créées en violant les conditions d'utilisation d'un site Web.

## Alors… Quand dois-je utiliser le grattage d'écran?

- Il est à présent clair, espérons-le, que le grattage d'écran peut souvent poser plus de problèmes que ce qu'elle vaut, en particulier à l'ère des interfaces de programmation d'applications (API). 
- Pourtant, il y a des cas où la capture d'écran reste utile. 
- Un cas d'utilisation idéal est celui où vous grattez différentes pages d'un même site Web. Un exemple pourrait inclure un site Web du gouvernement local qui publie des informations sur les évènements. 
- Un tel site Web peut avoir la même URL «racine», mais différents suffixes qui décrivent différents jours, mois, etc. 
- En supposant que la structure des pages est identique, on peut alors écrire une boucle qui commute la fin de l'URL pour différentes dates (en utilisant la convention de dénomination utilisée par le site).

## Alors… Quand dois-je utiliser le grattage d'écran?

- Une autre raison courante de gratter une page Web est qu'il est extrêmement difficile de copier et coller les informations que vous recherchez sur un site Web. 
- Les exemples sont des morceaux de texte extrêmement longs, ou ceux qui sont incorporés dans des tableaux complexes (mais encore une fois, l'analyse du code HTML pour identifier le texte précis dont vous avez besoin peut également prendre beaucoup de temps). 
- Dans de tels cas, il peut être utile d'utiliser les employés d'Amazon Mechanical Turk comme une approche alternative pour collecter vos données.





