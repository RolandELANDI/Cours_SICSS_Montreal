---
title: "Machine learning"
author: "Visseho Adjiwanou, PhD."
date: "19/06/2020"
output: html_document
---



```{r}

rm(list = ls())

#install.packages("caret")
#install.packages("farver")
#install.packages("e1071")
# nstall.packages("ggalt")  # Additional geom to ggplot

library(caret)
library(farver)
library(e1071) # ```{r} I removed the comment sing for e1071 and RSNNS as they are needed to train function
library(RSNNS)
library(tidyverse)
library(summarytools)
```


```{r}

data(iris)
dataset <- iris

# create a list of 70% of the rows in the original dataset we can use for training
test_index <- createDataPartition(dataset$Species, p=0.70, list=FALSE)

# select 30% of the data for validation
test <- dataset[-test_index, ]

# use the remaining 70% of data to training and testing the models
dataset <- dataset[test_index, ]

```

```{r}

# dimensions of dataset
dim(dataset)

# list types for each attribute
sapply(dataset, class)


# take a peek at the first 5 rows of the data
head(dataset)

# Levels of class
levels(dataset$Species)


# Class distribution
freq(dataset$Species)

# Statistical summary
summary(dataset)

stat.sum <-
  dataset %>% 
  select(- Species) %>% 
  summarise_all(~mean(., na.rm = T)) 

stat.sum

```

```{r}
library(ggalt)
# density plots for each attribute by class value
# split input and output
x <- dataset[,1:4]
y <- dataset[,5]
scales <- list(x=list(relation="free"), y=list(relation="free"))
featurePlot(x=x, y=y, plot="density", scales=scales)

ggplot(dataset) +
  geom_density(aes(x = Petal.Length, group = Species, color = Species))


dataset_long <-
  dataset %>% 
  pivot_longer(cols = Sepal.Length:Petal.Width, names_to = "Characteristics", values_to = "value")

x <- dataset[,1:4]
y <- dataset[,5]
scales <- list(x=list(relation="free"), y=list(relation="free"))
featurePlot(x=x, y=y, plot="density", scales=scales)

ggplot(dataset_long) +
  geom_density(aes( x = value, group = Species, color = Species)) +
  facet_wrap(~Characteristics, scales = "free") 


# Scatter plot

ggplot(dataset) +
  geom_point(aes(x = Petal.Length, y = Petal.Width, color = Species)) +
  geom_encircle(aes(x = Petal.Length, y = Petal.Width, color = Species))

```

```{r}

# Run algorithms using 10-fold cross validation
control <- trainControl(method="cv", number=10)
metric <- "Accuracy"


# logistic regression
set.seed(7)
fit.lr <- train(Species~., data=dataset, method="multinom", metric=metric, trControl=control)

# Random Forest
set.seed(7)
fit.rf <- train(Species~., data=dataset, method="rf", metric=metric, trControl=control)

# Neural network (i.e., a multi-layer perceptron)
set.seed(7)
fit.mlp <- train(Species~., data=dataset, method="mlp", metric=metric, trControl=control)

```

```{r}
library(tm)
# Load dataset of Trump's tweets

load(url("https://cbail.github.io/Trump_Tweets.Rdata"))

# list types for each attribute
sapply(trumptweets, class)

```

```{r}


# Turn the trump dataset into a corpus object
trump_corpus <- Corpus(VectorSource(as.vector(trumptweets$text))) 

#remove the puncutation
trump_corpus <- tm_map(trump_corpus, content_transformer(removePunctuation))

# 2. Stop word   

data("stop_words")
  
#trump_corpus <- tm_map(trump_corpus, removeWords, stopwords("english"))


# convert to term-document matrix, excluding words that occur less than twice
trump_DTM <- DocumentTermMatrix(trump_corpus, control = list(wordLengths = c(2, Inf), bounds=list(local=c(2,Inf))))
trump_DF <- data.frame(as.matrix(trump_DTM), stringsAsFactors=FALSE)


```

```{r}

# Now, suppose we want to predict whether the tweet was a retweet..
# We need to add this column to our processed text data

trump_DF$is_retweet <- as.factor(trumptweets$is_retweet)
levels(trump_DF$is_retweet) <- c("no", "yes")

frequency(trump_DF$is_retweet) # ```{r} I changed freq to frequency. I don't know why freq isn't working on my machine. 

set.seed(7)

getAnywhere(train)
fit.lr <- train(is_retweet~., data=trump_DF, method="multinom", metric=metric, trControl=control)

fit.lr
```

## Let evaluate a classification model

```{r}

# Run algorithms using 3-fold cross validation, for less compute time
# Let's use AUC as the metric, which requires setting classProbs=TRUE

control <- trainControl(method="cv", number=3, summaryFunction = twoClassSummary, classProbs=TRUE)
metric <- "ROC"

```

